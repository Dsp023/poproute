# AI/ML/LLM Glossary

Comprehensive terminology reference for artificial intelligence, machine learning, large language models, and related technologies.

---

## A

**Activation Function**: Non-linear function applied to neuron output in neural networks (ReLU, Sigmoid, Tanh).

**Adam Optimizer**: Adaptive moment estimation - popular optimization algorithm combining momentum and adaptive learning rates.

**Adversarial Examples**: Inputs designed to fool machine learning models.

**AGI (Artificial General Intelligence)**: Hypothetical AI with human-level intelligence across all domains.

**AI (Artificial Intelligence)**: Simulation of human intelligence by computer systems.

**Alignment**: Ensuring AI systems behave according to human values and intentions.

**Attention Mechanism**: Method allowing models to focus on relevant parts of input. Foundation of Transformers.

**Autoencoder**: Neural network learning compressed representations of data.

**AUC (Area Under Curve)**: Metric measuring classifier performance across all thresholds.

---

## B

**Backpropagation**: Algorithm for training neural networks by computing gradients.

**Batch normalization**: Normalizing layer inputs to stabilize and speed up training.

**Batch Size**: Number of training examples processed before updating model weights.

**Bayesian neural networks**: Neural networks with probability distributions over weights.

**BERT (Bidirectional Encoder Representations from Transformers)**: Google's language model for understanding text.

**Bias (ML)**: Error from wrong assumptions. High bias = underfitting.

**Bias (Data)**: Systematic errors in training data leading to unfair model behavior.

**Bidirectional**: Processing sequence in both forward and backward directions.

---

## C

**Causal inference**: Determining cause-and-effect relationships from data.

**Chain-of-thought**: Prompting technique asking LLM to reason step-by-step.

**ChatGPT**: OpenAI's conversational AI based on GPT models.

**Checkpoint**: Saved model state during training.

**Classification**: Predicting discrete categories/labels.

**Claude**: Anthropic's large language model.

**CNN (Convolutional Neural Network)**: Deep learning architecture for image/spatial data.

**Context window**: Maximum input length an LLM can process.

**Convolution**: Operation sliding filter over data to detect patterns (images, audio).

**Cross-entropy**: Loss function for classification tasks.

**Cross-validation**: Technique splitting data multiple ways for reliable evaluation.

---

## D

**Data augmentation**: Creating variations of training data (flips, rotations, etc.).

**Deep learning**: Machine learning using multi-layer neural networks.

**Deployment**: Making trained model available for real-world use.

**Diffusion models**: Generative models learning to denoise data (Stable Diffusion, DALL-E 2).

**Discriminator**: Network distinguishing real from fake data (in GANs).

**Dropout**: Regularization technique randomly dropping neurons during training.

**Data drift**: Changes in data distribution over time affecting model performance.

---

## E

**Embedding**: Dense vector representation of data (text, images) capturing semantic meaning.

**Encoder**: Component transforming input to latent representation.

**Encoder-decoder**: Architecture with separate encoding and decoding components (Transformers).

**Ensemble**: Combining multiple models for better predictions.

**Epoch**: One complete pass through entire training dataset.

**Expert system**: AI using rules to mimic human expert decision-making.

---

## F

**F1 Score**: Harmonic mean of precision and recall.

**Feature**: Individual measurable property/characteristic in data.

**Feature engineering**: Creating new features from existing data.

**Few-shot learning**: Learning from few examples.

**Fine-tuning**: Further training pre-trained model on specific task/dataset.

**Forward propagation**: Computing neural network output from input.

---

## G

**GAN (Generative Adversarial Network)**: Two networks competing - generator creates, discriminator evaluates.

**Gemini**: Google's multimodal large language model.

**Generative AI**: AI creating new content (text, images, audio, video).

**GPT (Generative Pre-trained Transformer)**: OpenAI's language model series.

**Gradient descent**: Optimization algorithm iteratively adjusting parameters to minimize loss.

**Gradient**: Derivative showing direction to adjust parameters.

**Ground truth**: Actual correct answer/label in dataset.

---

## H

**Hallucination**: when LLM generates plausible but false information.

**Hidden layer**: Neural network layer between input and output.

**Hugging Face**: Platform for sharing and using AI models, especially Transformers.

**Hyperparameter**: Model configuration set before training (learning rate, layer size).

**Hyperparameter tuning**: Finding optimal hyperparameter values.

---

## I

**Inference**: Using trained model to make predictions.

**Input layer**: First layer of neural network receiving input data.

**Instance segmentation**: Identifying and separating individual objects in images.

---

## K

**K-Fold cross-validation**: Splitting data into K parts for validation.

**K-means clustering**: Algorithm grouping data into K clusters.

**KNN (K-Nearest Neighbors)**: Algorithm classifying based on K nearest training examples.

**Knowledge distillation**: Training smaller model to mimic larger model.

**Knowledge graph**: Structured representation of entities and relationships.

---

## L

**Label**: Correct answer/category for training example (in supervised learning).

**Latent space**: Compressed representation learned by model (autoencoder).

**Learning rate**: Step size for gradient descent updates.

**LLaMA**: Meta's open-source large language model.

**LLM (Large Language Model)**: AI model trained on vast text data to understand/generate language.

**Logistic regression**: Algorithm for binary classification outputting probabilities.

**Loss function**: Measures how wrong model predictions are. Goal: minimize loss.

**LSTM (Long Short-Term Memory)**: RNN variant handling long sequences.

---

## M

**MAE (Mean Absolute Error)**: Average absolute difference between predictions and actual values.

**MAML (Model-Agnostic Meta-Learning)**: Algorithm learning to adapt quickly to new tasks.

**MDP (Markov Decision Process)**: Framework for sequential decision problems.

**Meta-learning**: Learning to learn - improving at learning new tasks.

**Mistral**: Open-source high-performance large language model.

**MLOps**: Practices for deploying and maintaining ML systems in production.

**MSE (Mean Squared Error)**: Average squared difference between predictions and actual values.

**Multi-head attention**: Multiple attention mechanisms in parallel (in Transformers).

---

## N

**NER (Named Entity Recognition)**: Identifying and classifying named entities in text.

**Neural network**: Computing system inspired by biological neurons.

**NLP (Natural Language Processing)**: AI understanding and generating human language.

**Normalization**: Scaling data to standard range (e.g., [0,1]).

---

## O

**Object detection**: Finding and classifying objects in images with bounding boxes.

**Ollama**: Tool for running large language models locally.

**One-hot encoding**: Representing categories as binary vectors.

**Optimization**: Process of finding best model parameters.

**Output layer**: Final neural network layer producing predictions.

**Overfitting**: Model memorizes training data, performs poorly on new data.

---

## P

**Parameter**: Model weight learned from training data.

**PCA (Principal Component Analysis)**: Dimensionality reduction finding directions of maximum variance.

**Perceptron**: Single-layer neural network (or single neuron).

**Pooling**: Downsampling operation in CNNs (max pooling, average pooling).

**Positional encoding**: Adding position information to tokens (in Transformers).

**Pre-training**: Initial training on large dataset before fine-tuning.

**Precision**: Of positive predictions, how many were correct. TP/(TP+FP).

**Prompt**: Input instruction/query given to large language model.

**Prompt engineering**: Crafting effective prompts for LLMs.

---

## Q

**Q-learning**: Reinforcement learning algorithm learning action-value function.

**Quantization**: Reducing model precision (e.g., 32-bit → 8-bit) for efficiency.

---

## R

**RAG (Retrieval Augmented Generation)**: Enhancing LLM with retrieved information from external sources.

**Random forest**: Ensemble of decision trees.

**Recall**: Of actual positives, how many were found. TP/(TP+FN).

**Recurrent neural network (RNN)**: Neural network for sequential data with loops.

**Regression**: Predicting continuous numerical values.

**Regularization**: Techniques preventing overfitting (L1, L2, dropout).

**Reinforcement learning**: Learning through interaction with environment via rewards.

**ReLU (Rectified Linear Unit)**: Activation function f(x) = max(0, x).

**ResNet**: Deep CNN architecture with skip connections.

**RLHF (Reinforcement Learning from Human Feedback)**: Training AI using human preferences.

**Rollout**: Using model in production environment.

---

## S

**Semantic search**: Finding information based on meaning, not just keywords.

**Semantic segmentation**: Labeling every pixel in image.

**Sentiment analysis**: Determining emotional tone of text.

**Sequence-to-sequence (Seq2Seq)**: Model transforming input sequence to output sequence.

**Softmax**: Activation function converting values to probability distribution.

**Standardization**: Scaling data to mean=0, std=1.

**Supervised learning**: Learning from labeled examples (input-output pairs).

**SVM (Support Vector Machine)**: Algorithm finding optimal decision boundary.

---

## T

**Temperature**: Parameter controlling randomness in LLM generation (higher = more random).

**TensorFlow**: Google's deep learning framework.

**Tensor**: Multi-dimensional array (generalization of matrix).

**Test set**: Data held out for final model evaluation.

**Tokenization**: Splitting text into tokens (words/subwords).

**Training set**: Data used to train model.

**Transfer learning**: Using knowledge from one task to improve learning on another.

**Transformer**: Neural architecture based on self-attention (powers modern LLMs).

**t-SNE**: Dimensionality reduction technique for visualization.

---

## U

**Underfitting**: Model too simple, doesn't capture patterns (high bias).

**Unsupervised learning**: Learning from unlabeled data (clustering, dimensionality reduction).

---

## V

**Validation set**: Data used to tune hyperparameters and select model.

**Vanishing gradient**: Gradients become tiny, preventing deep network training.

**Variance**: Error from sensitivity to training data fluctuations. High variance = overfitting.

**VAE (Variational Autoencoder)**: Generative model learning probabilistic latent space.

**Vector database**: Database optimized for storing and searching vector embeddings.

**VGG**: Deep CNN architecture with uniform 3×3 convolutions.

---

## W

**Weight**: Learnable parameter in neural network.

**Word embedding**: Dense vector representation of word capturing meaning (Word2Vec, GloVe).

---

## X

**XGBoost**: Gradient boosting library with high performance.

---

## Z

**Zero-shot learning**: Performing task without task-specific training examples.

---

## Numbers

**1-NN**: K-Nearest Neighbors with K=1.

---

*For more detailed explanations, see the topic-specific guides in this repository.*
